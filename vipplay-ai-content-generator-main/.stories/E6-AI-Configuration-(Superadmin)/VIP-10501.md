# VIP-10501: List Ollama Models

**Story ID**: VIP-10501
**Story Type**: Story
**Epic**: E6-AI-Configuration-(Superadmin)
**Priority**: High
**Story Points**: 3
**Status**: ✅ Complete

## User Story

As a superadmin, I want to see all available Ollama models so that I know what's installed

## Acceptance Criteria

- [x] **AC1**: Superadmin can list all available Ollama models via API endpoint
- [x] **AC2**: API returns model list with name, size, and modification date
- [x] **AC3**: Non-superadmin users receive 403 Forbidden error
- [x] **AC4**: Unauthenticated requests receive 401 Unauthorized error
- [x] **AC5**: API handles FastAPI service unavailability gracefully (503 error)

## Technical Details

**API Endpoint**: `GET /api/admin/ai/models`

**Authentication**: Requires JWT token with `superadmin` role

**Data Flow**:
```
Next.js API → FastAPI: GET /models → Ollama API → FastAPI → Next.js API → Frontend
```

**Implementation Files**:
- `app/api/admin/ai/models/route.ts` - Next.js API route (proxies to FastAPI)
- `api-service/main.py` - FastAPI endpoint (line 79-89)
- `api-service/services/ollama_service.py` - Ollama service integration

**Response Format**:
```json
{
  "models": [
    {
      "name": "llama3.1:8b",
      "size": 4730000000,
      "modified": "2025-01-15T10:30:00Z",
      "digest": "sha256:..."
    }
  ],
  "count": 3,
  "fastapiUrl": "http://localhost:8000"
}
```

**Error Responses**:
- `401 Unauthorized` - Missing or invalid token
- `403 Forbidden` - User is not superadmin
- `503 Service Unavailable` - FastAPI or Ollama unavailable

## Definition of Done

- [x] Code implemented and working
- [x] All acceptance criteria met
- [x] E2E tests written and passing (tests/e2e/e6-ai-configuration.spec.ts)
- [x] Code reviewed and approved
- [x] Merged to main branch
- [x] Documented (ARCHITECTURE.md updated)

## Notes

**Implementation Notes**:
- Next.js API route proxies to FastAPI service (not direct Ollama calls)
- FastAPI endpoint: `GET /models` calls Ollama service
- No mock data - uses real Ollama API via FastAPI
- Response includes `fastapiUrl` to indicate service used
- FastAPI Swagger available at `http://localhost:8000/docs`
