# VIP-10209: SEO Analysis & Scoring System

**Story ID**: VIP-10209
**Story Type**: Story
**Epic**: E3-Content-Generation-(AI)
**Priority**: High
**Story Points**: 5
**Status**: To Do

## User Story

As a content creator, I want comprehensive SEO analysis and scoring for all generated content so that I can understand optimization quality and identify areas for improvement to maximize search engine visibility

## Acceptance Criteria

- [ ] **AC1**: Calculate overall SEO score (0-100) with weighted component breakdown
- [ ] **AC2**: Analyze keyword optimization (density, placement, prominence)
- [ ] **AC3**: Evaluate content structure (headings hierarchy, meta data quality)
- [ ] **AC4**: Assess technical SEO factors (URL structure, internal linking suggestions)
- [ ] **AC5**: Provide actionable recommendations for improvement
- [ ] **AC6**: Store SEO analysis in MongoDB for historical tracking
- [ ] **AC7**: Display SEO score with visual breakdown in Next.js UI

## Technical Details

**Architecture:**
- **FastAPI Role**: Calculate SEO metrics, analyze content structure, generate recommendations
- **Next.js Role**: Store SEO analysis in MongoDB, display scores in UI, track improvements over time
- **Data Flow**: Next.js → FastAPI (content for analysis) → SEO metrics → Next.js → MongoDB

**SEO Analysis Workflow:**

```
Generated Content (from VIP-10204-10207):
- Article markdown content
- Meta title and description
- Keywords (primary, secondary, LSI)
- Word count and reading time
    ↓
Next.js calls FastAPI /api/seo/analyze
    ↓
FastAPI SEOAnalysisService.analyze_content()
    ↓
Component 1: Keyword Analysis (30 points)
  → Primary keyword density (target: 1-2%)
  → Primary keyword placement (title, H1, first 100 words, conclusion)
  → Secondary keyword distribution
  → LSI keyword usage
  → Keyword prominence score
    ↓
Component 2: Content Structure (25 points)
  → Heading hierarchy (H1 → H2 → H3)
  → Paragraph length consistency
  → Content depth and comprehensiveness
  → Internal linking opportunities
  → Image alt text suggestions
    ↓
Component 3: Meta Data Quality (20 points)
  → Meta title optimization (length, keyword placement)
  → Meta description quality (length, CTA, keywords)
  → URL slug SEO-friendliness
  → Schema markup recommendations
    ↓
Component 4: Readability & Engagement (15 points)
  → Flesch-Kincaid Reading Ease
  → Sentence length variation
  → Paragraph structure
  → Bullet points and lists usage
    ↓
Component 5: Technical SEO (10 points)
  → Content length adequacy (500+ words)
  → Keyword stuffing detection
  → Duplicate content check
  → Mobile-friendliness indicators
    ↓
Calculate Overall SEO Score (0-100)
    ↓
Generate Recommendations
    ↓
FastAPI returns SEO analysis JSON to Next.js
    ↓
Next.js stores in MongoDB generated_content collection
    ↓
Display in UI with visual breakdown
```

**SEO Analysis Service (`services/seo_analysis_service.py` - NEW):**

```python
"""
SEO Analysis Service
Comprehensive SEO scoring and analysis for generated content
"""

import re
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
import logging

logger = logging.getLogger(__name__)


@dataclass
class SEOAnalysisResult:
    """SEO analysis result data class"""
    overall_score: int  # 0-100
    component_scores: Dict[str, float]
    keyword_analysis: Dict
    content_structure: Dict
    meta_data_quality: Dict
    readability_metrics: Dict
    technical_seo: Dict
    recommendations: List[str]
    strengths: List[str]
    weaknesses: List[str]


class SEOAnalysisService:
    """Service for comprehensive SEO analysis"""

    def __init__(self):
        self.weights = {
            "keyword_optimization": 0.30,
            "content_structure": 0.25,
            "meta_data_quality": 0.20,
            "readability_engagement": 0.15,
            "technical_seo": 0.10
        }

    async def analyze_content(
        self,
        content: str,
        meta_title: str,
        meta_description: str,
        primary_keyword: str,
        secondary_keywords: List[str],
        lsi_keywords: Optional[List[str]] = None,
        url_slug: Optional[str] = None
    ) -> SEOAnalysisResult:
        """
        Perform comprehensive SEO analysis

        Args:
            content: Full article markdown content
            meta_title: Meta title
            meta_description: Meta description
            primary_keyword: Primary target keyword
            secondary_keywords: Secondary keywords
            lsi_keywords: LSI keywords
            url_slug: URL slug

        Returns:
            SEOAnalysisResult with scores and recommendations
        """
        logger.info(f"Analyzing SEO for content with primary keyword: {primary_keyword}")

        # Component 1: Keyword Analysis (30 points)
        keyword_score, keyword_analysis = await self._analyze_keywords(
            content, meta_title, meta_description,
            primary_keyword, secondary_keywords, lsi_keywords or []
        )

        # Component 2: Content Structure (25 points)
        structure_score, structure_analysis = await self._analyze_structure(
            content
        )

        # Component 3: Meta Data Quality (20 points)
        meta_score, meta_analysis = await self._analyze_meta_data(
            meta_title, meta_description, url_slug, primary_keyword
        )

        # Component 4: Readability & Engagement (15 points)
        readability_score, readability_analysis = await self._analyze_readability(
            content
        )

        # Component 5: Technical SEO (10 points)
        technical_score, technical_analysis = await self._analyze_technical_seo(
            content, primary_keyword, secondary_keywords
        )

        # Calculate weighted overall score
        overall_score = int(
            keyword_score * self.weights["keyword_optimization"] +
            structure_score * self.weights["content_structure"] +
            meta_score * self.weights["meta_data_quality"] +
            readability_score * self.weights["readability_engagement"] +
            technical_score * self.weights["technical_seo"]
        )

        # Generate recommendations
        recommendations = self._generate_recommendations(
            overall_score, keyword_analysis, structure_analysis,
            meta_analysis, readability_analysis, technical_analysis
        )

        # Identify strengths and weaknesses
        strengths, weaknesses = self._identify_strengths_weaknesses(
            keyword_score, structure_score, meta_score,
            readability_score, technical_score
        )

        return SEOAnalysisResult(
            overall_score=overall_score,
            component_scores={
                "keyword_optimization": keyword_score,
                "content_structure": structure_score,
                "meta_data_quality": meta_score,
                "readability_engagement": readability_score,
                "technical_seo": technical_score
            },
            keyword_analysis=keyword_analysis,
            content_structure=structure_analysis,
            meta_data_quality=meta_analysis,
            readability_metrics=readability_analysis,
            technical_seo=technical_analysis,
            recommendations=recommendations,
            strengths=strengths,
            weaknesses=weaknesses
        )

    async def _analyze_keywords(
        self,
        content: str,
        meta_title: str,
        meta_description: str,
        primary_keyword: str,
        secondary_keywords: List[str],
        lsi_keywords: List[str]
    ) -> Tuple[float, Dict]:
        """
        Analyze keyword optimization (30 points max)

        Scoring breakdown:
        - Primary keyword density (1-2%): 8 points
        - Primary keyword placement (title, H1, intro, conclusion): 10 points
        - Secondary keyword distribution: 6 points
        - LSI keyword usage: 4 points
        - Keyword prominence: 2 points
        """
        content_lower = content.lower()
        word_count = len(content.split())

        # Primary keyword analysis
        primary_count = content_lower.count(primary_keyword.lower())
        primary_density = (primary_count / word_count) * 100 if word_count > 0 else 0

        # Primary keyword placement
        in_title = primary_keyword.lower() in meta_title.lower()
        in_h1 = self._check_h1_keyword(content, primary_keyword)
        in_first_100 = primary_keyword.lower() in ' '.join(content.split()[:100]).lower()
        in_conclusion = primary_keyword.lower() in ' '.join(content.split()[-100:]).lower()

        # Secondary keyword distribution
        secondary_usage = sum(
            1 for kw in secondary_keywords
            if kw.lower() in content_lower
        )

        # LSI keyword usage
        lsi_usage = sum(
            1 for kw in lsi_keywords
            if kw.lower() in content_lower
        )

        # Calculate score
        score = 0.0

        # Primary density scoring (8 points)
        if 1.0 <= primary_density <= 2.0:
            score += 8.0
        elif 0.5 <= primary_density < 1.0 or 2.0 < primary_density <= 2.5:
            score += 5.0
        elif primary_density > 0:
            score += 2.0

        # Placement scoring (10 points)
        if in_title:
            score += 3.0
        if in_h1:
            score += 2.5
        if in_first_100:
            score += 2.5
        if in_conclusion:
            score += 2.0

        # Secondary keywords (6 points)
        secondary_ratio = secondary_usage / len(secondary_keywords) if secondary_keywords else 0
        score += secondary_ratio * 6.0

        # LSI keywords (4 points)
        lsi_ratio = lsi_usage / len(lsi_keywords) if lsi_keywords else 0
        score += lsi_ratio * 4.0

        # Keyword prominence (2 points)
        if in_title and in_h1 and in_first_100:
            score += 2.0
        elif (in_title and in_h1) or (in_title and in_first_100):
            score += 1.0

        return score, {
            "primary_keyword": primary_keyword,
            "primary_count": primary_count,
            "primary_density": round(primary_density, 2),
            "primary_density_optimal": 1.0 <= primary_density <= 2.0,
            "placements": {
                "in_title": in_title,
                "in_h1": in_h1,
                "in_first_100_words": in_first_100,
                "in_conclusion": in_conclusion
            },
            "secondary_keywords_used": secondary_usage,
            "secondary_keywords_total": len(secondary_keywords),
            "lsi_keywords_used": lsi_usage,
            "lsi_keywords_total": len(lsi_keywords),
            "score": round(score, 1)
        }

    def _check_h1_keyword(self, content: str, keyword: str) -> bool:
        """Check if keyword appears in H1 heading"""
        h1_match = re.search(r'^#\s+(.+)$', content, re.MULTILINE)
        if h1_match:
            h1_text = h1_match.group(1).lower()
            return keyword.lower() in h1_text
        return False

    async def _analyze_structure(self, content: str) -> Tuple[float, Dict]:
        """
        Analyze content structure (25 points max)

        Scoring breakdown:
        - Proper heading hierarchy: 8 points
        - Adequate heading count: 5 points
        - Paragraph length consistency: 5 points
        - Content depth: 4 points
        - List/bullet usage: 3 points
        """
        score = 0.0

        # Extract headings
        h1_count = len(re.findall(r'^#\s+', content, re.MULTILINE))
        h2_count = len(re.findall(r'^##\s+', content, re.MULTILINE))
        h3_count = len(re.findall(r'^###\s+', content, re.MULTILINE))

        # Heading hierarchy (8 points)
        if h1_count == 1:  # Exactly one H1
            score += 4.0
        if h2_count >= 3:  # At least 3 H2s
            score += 3.0
        if h3_count > 0:  # Some H3s for depth
            score += 1.0

        # Adequate heading count (5 points)
        total_headings = h2_count + h3_count
        if total_headings >= 5:
            score += 5.0
        elif total_headings >= 3:
            score += 3.0

        # Paragraph analysis
        paragraphs = [p.strip() for p in content.split('\n\n') if p.strip() and not p.strip().startswith('#')]
        avg_paragraph_words = sum(len(p.split()) for p in paragraphs) / len(paragraphs) if paragraphs else 0

        # Paragraph length (5 points)
        if 50 <= avg_paragraph_words <= 100:
            score += 5.0
        elif 30 <= avg_paragraph_words < 50 or 100 < avg_paragraph_words <= 150:
            score += 3.0

        # Content depth (4 points)
        word_count = len(content.split())
        if word_count >= 1500:
            score += 4.0
        elif word_count >= 1000:
            score += 3.0
        elif word_count >= 500:
            score += 2.0

        # List usage (3 points)
        bullet_lists = len(re.findall(r'^\s*[-*+]\s+', content, re.MULTILINE))
        numbered_lists = len(re.findall(r'^\s*\d+\.\s+', content, re.MULTILINE))
        if bullet_lists + numbered_lists >= 2:
            score += 3.0
        elif bullet_lists + numbered_lists >= 1:
            score += 2.0

        return score, {
            "heading_hierarchy": {
                "h1_count": h1_count,
                "h2_count": h2_count,
                "h3_count": h3_count,
                "proper_hierarchy": h1_count == 1 and h2_count >= 3
            },
            "paragraphs": {
                "count": len(paragraphs),
                "average_words": round(avg_paragraph_words, 1),
                "optimal_length": 50 <= avg_paragraph_words <= 100
            },
            "content_depth": {
                "word_count": word_count,
                "adequate_depth": word_count >= 1000
            },
            "lists": {
                "bullet_lists": bullet_lists,
                "numbered_lists": numbered_lists
            },
            "score": round(score, 1)
        }

    async def _analyze_meta_data(
        self,
        meta_title: str,
        meta_description: str,
        url_slug: Optional[str],
        primary_keyword: str
    ) -> Tuple[float, Dict]:
        """
        Analyze meta data quality (20 points max)

        Scoring breakdown:
        - Meta title optimization: 8 points
        - Meta description quality: 8 points
        - URL slug optimization: 4 points
        """
        score = 0.0

        # Meta title analysis
        title_length = len(meta_title)
        title_has_keyword = primary_keyword.lower() in meta_title.lower()
        title_keyword_position = meta_title.lower().find(primary_keyword.lower()) if title_has_keyword else -1

        # Meta title scoring (8 points)
        if 50 <= title_length <= 60:
            score += 4.0
        elif 40 <= title_length < 50 or 60 < title_length <= 70:
            score += 2.0

        if title_has_keyword:
            score += 3.0
            if title_keyword_position < 20:  # Keyword early in title
                score += 1.0

        # Meta description analysis
        desc_length = len(meta_description)
        desc_has_keyword = primary_keyword.lower() in meta_description.lower()
        desc_has_cta = any(cta in meta_description.lower() for cta in ['learn', 'discover', 'find out', 'explore', 'get'])

        # Meta description scoring (8 points)
        if 150 <= desc_length <= 160:
            score += 4.0
        elif 140 <= desc_length < 150 or 160 < desc_length <= 170:
            score += 2.0

        if desc_has_keyword:
            score += 2.0
        if desc_has_cta:
            score += 2.0

        # URL slug analysis (4 points)
        if url_slug:
            slug_has_keyword = primary_keyword.lower().replace(' ', '-') in url_slug.lower()
            slug_length = len(url_slug)
            slug_clean = re.match(r'^[a-z0-9-]+$', url_slug) is not None

            if slug_has_keyword:
                score += 2.0
            if slug_clean and slug_length <= 60:
                score += 2.0

        return score, {
            "meta_title": {
                "length": title_length,
                "optimal_length": 50 <= title_length <= 60,
                "has_keyword": title_has_keyword,
                "keyword_position": title_keyword_position if title_has_keyword else None
            },
            "meta_description": {
                "length": desc_length,
                "optimal_length": 150 <= desc_length <= 160,
                "has_keyword": desc_has_keyword,
                "has_cta": desc_has_cta
            },
            "url_slug": {
                "value": url_slug,
                "has_keyword": slug_has_keyword if url_slug else None,
                "clean_format": slug_clean if url_slug else None
            } if url_slug else None,
            "score": round(score, 1)
        }

    async def _analyze_readability(self, content: str) -> Tuple[float, Dict]:
        """
        Analyze readability and engagement (15 points max)

        Scoring breakdown:
        - Flesch Reading Ease score: 8 points
        - Sentence length variation: 4 points
        - Paragraph structure: 3 points
        """
        score = 0.0

        # Calculate Flesch Reading Ease (simplified)
        words = content.split()
        sentences = re.split(r'[.!?]+', content)
        sentences = [s for s in sentences if s.strip()]

        word_count = len(words)
        sentence_count = len(sentences)
        syllable_count = self._estimate_syllables(content)

        if word_count > 0 and sentence_count > 0:
            avg_words_per_sentence = word_count / sentence_count
            avg_syllables_per_word = syllable_count / word_count

            flesch_score = 206.835 - (1.015 * avg_words_per_sentence) - (84.6 * avg_syllables_per_word)
            flesch_score = max(0, min(100, flesch_score))  # Clamp to 0-100

            # Flesch scoring (8 points)
            if flesch_score >= 60:  # Easy to read
                score += 8.0
            elif flesch_score >= 50:  # Fairly easy
                score += 6.0
            elif flesch_score >= 40:  # Average
                score += 4.0
            else:
                score += 2.0

            # Sentence length variation (4 points)
            sentence_lengths = [len(s.split()) for s in sentences if s.strip()]
            if sentence_lengths:
                length_variance = max(sentence_lengths) - min(sentence_lengths)
                if length_variance >= 15:  # Good variation
                    score += 4.0
                elif length_variance >= 10:
                    score += 2.0

            # Paragraph structure (3 points)
            paragraphs = [p for p in content.split('\n\n') if p.strip()]
            if len(paragraphs) >= 5:
                score += 3.0
            elif len(paragraphs) >= 3:
                score += 2.0

        else:
            flesch_score = 0
            avg_words_per_sentence = 0

        return score, {
            "flesch_reading_ease": round(flesch_score, 1),
            "readability_level": self._get_readability_level(flesch_score),
            "avg_words_per_sentence": round(avg_words_per_sentence, 1),
            "sentence_count": sentence_count,
            "paragraph_count": len(paragraphs) if 'paragraphs' in locals() else 0,
            "score": round(score, 1)
        }

    def _estimate_syllables(self, text: str) -> int:
        """Estimate syllable count (simplified)"""
        words = text.lower().split()
        syllable_count = 0
        for word in words:
            word = re.sub(r'[^a-z]', '', word)
            if not word:
                continue
            vowels = 'aeiouy'
            syllables = 0
            previous_was_vowel = False
            for char in word:
                is_vowel = char in vowels
                if is_vowel and not previous_was_vowel:
                    syllables += 1
                previous_was_vowel = is_vowel
            if word.endswith('e'):
                syllables -= 1
            if syllables == 0:
                syllables = 1
            syllable_count += syllables
        return syllable_count

    def _get_readability_level(self, flesch_score: float) -> str:
        """Get readability level from Flesch score"""
        if flesch_score >= 90:
            return "Very Easy (5th grade)"
        elif flesch_score >= 80:
            return "Easy (6th grade)"
        elif flesch_score >= 70:
            return "Fairly Easy (7th grade)"
        elif flesch_score >= 60:
            return "Standard (8th-9th grade)"
        elif flesch_score >= 50:
            return "Fairly Difficult (10th-12th grade)"
        elif flesch_score >= 30:
            return "Difficult (College)"
        else:
            return "Very Difficult (College graduate)"

    async def _analyze_technical_seo(
        self,
        content: str,
        primary_keyword: str,
        secondary_keywords: List[str]
    ) -> Tuple[float, Dict]:
        """
        Analyze technical SEO factors (10 points max)

        Scoring breakdown:
        - Content length adequacy: 4 points
        - Keyword stuffing detection: 3 points
        - Image alt text suggestions: 3 points
        """
        score = 0.0

        word_count = len(content.split())

        # Content length (4 points)
        if word_count >= 1500:
            score += 4.0
        elif word_count >= 1000:
            score += 3.0
        elif word_count >= 500:
            score += 2.0

        # Keyword stuffing detection (3 points)
        primary_density = (content.lower().count(primary_keyword.lower()) / word_count) * 100 if word_count > 0 else 0
        keyword_stuffing = primary_density > 3.0

        if not keyword_stuffing:
            score += 3.0
        elif primary_density <= 4.0:
            score += 1.0

        # Image suggestions (3 points) - placeholder for future
        # In real implementation, would check for image markdown and alt text
        image_count = len(re.findall(r'!\[.*?\]\(.*?\)', content))
        if image_count >= 2:
            score += 3.0
        elif image_count >= 1:
            score += 2.0

        return score, {
            "content_length": {
                "word_count": word_count,
                "adequate": word_count >= 500,
                "optimal": word_count >= 1000
            },
            "keyword_stuffing": {
                "detected": keyword_stuffing,
                "primary_density": round(primary_density, 2),
                "safe_range": primary_density <= 3.0
            },
            "images": {
                "count": image_count,
                "recommendation": "Add 2-3 relevant images with descriptive alt text" if image_count < 2 else "Good image usage"
            },
            "score": round(score, 1)
        }

    def _generate_recommendations(
        self,
        overall_score: int,
        keyword_analysis: Dict,
        structure_analysis: Dict,
        meta_analysis: Dict,
        readability_analysis: Dict,
        technical_analysis: Dict
    ) -> List[str]:
        """Generate actionable recommendations"""
        recommendations = []

        # Keyword recommendations
        if keyword_analysis["primary_density"] < 1.0:
            recommendations.append(
                f"Increase primary keyword '{keyword_analysis['primary_keyword']}' usage. "
                f"Current density: {keyword_analysis['primary_density']}%, target: 1-2%"
            )
        elif keyword_analysis["primary_density"] > 2.0:
            recommendations.append(
                f"Reduce primary keyword density to avoid stuffing. "
                f"Current: {keyword_analysis['primary_density']}%, optimal: 1-2%"
            )

        if not keyword_analysis["placements"]["in_title"]:
            recommendations.append("Include primary keyword in meta title")
        if not keyword_analysis["placements"]["in_h1"]:
            recommendations.append("Include primary keyword in H1 heading")

        # Structure recommendations
        if structure_analysis["heading_hierarchy"]["h1_count"] != 1:
            recommendations.append("Use exactly one H1 heading for proper hierarchy")
        if structure_analysis["heading_hierarchy"]["h2_count"] < 3:
            recommendations.append("Add more H2 headings (at least 3) to improve structure")

        # Meta data recommendations
        meta_title = meta_analysis["meta_title"]
        if not (50 <= meta_title["length"] <= 60):
            recommendations.append(
                f"Optimize meta title length (current: {meta_title['length']} chars, optimal: 50-60)"
            )

        meta_desc = meta_analysis["meta_description"]
        if not (150 <= meta_desc["length"] <= 160):
            recommendations.append(
                f"Optimize meta description length (current: {meta_desc['length']} chars, optimal: 150-160)"
            )

        # Readability recommendations
        if readability_analysis["flesch_reading_ease"] < 60:
            recommendations.append(
                "Improve readability by using shorter sentences and simpler words"
            )

        # Technical recommendations
        if technical_analysis["content_length"]["word_count"] < 1000:
            recommendations.append(
                f"Expand content (current: {technical_analysis['content_length']['word_count']} words, "
                "recommended: 1000+ words for better SEO)"
            )

        return recommendations

    def _identify_strengths_weaknesses(
        self,
        keyword_score: float,
        structure_score: float,
        meta_score: float,
        readability_score: float,
        technical_score: float
    ) -> Tuple[List[str], List[str]]:
        """Identify strengths and weaknesses"""
        strengths = []
        weaknesses = []

        components = {
            "Keyword Optimization": (keyword_score, 30),
            "Content Structure": (structure_score, 25),
            "Meta Data Quality": (meta_score, 20),
            "Readability": (readability_score, 15),
            "Technical SEO": (technical_score, 10)
        }

        for name, (score, max_score) in components.items():
            percentage = (score / max_score) * 100
            if percentage >= 80:
                strengths.append(f"{name}: {int(percentage)}%")
            elif percentage < 60:
                weaknesses.append(f"{name}: {int(percentage)}%")

        return strengths, weaknesses


# Singleton instance
seo_analysis_service = SEOAnalysisService()
```

**Update Content Router (`routers/content.py`):**

```python
from services.seo_analysis_service import seo_analysis_service, SEOAnalysisResult
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel, Field
from typing import List, Optional

router = APIRouter(prefix="/api/seo", tags=["seo"])


class SEOAnalysisRequest(BaseModel):
    """Request model for SEO analysis"""
    content: str = Field(..., min_length=100, description="Article content (markdown)")
    meta_title: str = Field(..., min_length=10, max_length=100)
    meta_description: str = Field(..., min_length=50, max_length=200)
    primary_keyword: str = Field(..., min_length=2, max_length=100)
    secondary_keywords: List[str] = Field(default=[], max_items=10)
    lsi_keywords: Optional[List[str]] = Field(None, max_items=20)
    url_slug: Optional[str] = Field(None, max_length=100)


class SEOAnalysisResponse(BaseModel):
    """Response model for SEO analysis"""
    overall_score: int
    component_scores: dict
    keyword_analysis: dict
    content_structure: dict
    meta_data_quality: dict
    readability_metrics: dict
    technical_seo: dict
    recommendations: List[str]
    strengths: List[str]
    weaknesses: List[str]


@router.post("/analyze", response_model=SEOAnalysisResponse)
async def analyze_seo(request: SEOAnalysisRequest):
    """
    Analyze content for SEO optimization

    Performs comprehensive SEO analysis including:
    - Keyword optimization (density, placement, prominence)
    - Content structure (headings, paragraphs, depth)
    - Meta data quality (title, description, URL)
    - Readability (Flesch-Kincaid, sentence length)
    - Technical SEO (length, keyword stuffing, images)

    Returns:
    - Overall SEO score (0-100)
    - Component scores with detailed metrics
    - Actionable recommendations for improvement
    """
    try:
        result: SEOAnalysisResult = await seo_analysis_service.analyze_content(
            content=request.content,
            meta_title=request.meta_title,
            meta_description=request.meta_description,
            primary_keyword=request.primary_keyword,
            secondary_keywords=request.secondary_keywords,
            lsi_keywords=request.lsi_keywords,
            url_slug=request.url_slug
        )

        return SEOAnalysisResponse(
            overall_score=result.overall_score,
            component_scores=result.component_scores,
            keyword_analysis=result.keyword_analysis,
            content_structure=result.content_structure,
            meta_data_quality=result.meta_data_quality,
            readability_metrics=result.readability_metrics,
            technical_seo=result.technical_seo,
            recommendations=result.recommendations,
            strengths=result.strengths,
            weaknesses=result.weaknesses
        )

    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"SEO analysis failed: {str(e)}"
        )
```

**Integration with Next.js (`app/api/content/seo/route.ts` - NEW):**

```typescript
import { NextRequest, NextResponse } from 'next/server';
import { getServerSession } from 'next-auth';
import { authOptions } from '@/lib/auth';
import { getDb } from '@/lib/mongodb';
import { ObjectId } from 'mongodb';

/**
 * POST /api/content/seo
 * Analyze generated content for SEO and store results
 */
export async function POST(request: NextRequest) {
  const session = await getServerSession(authOptions);
  if (!session) {
    return NextResponse.json({ error: 'Unauthorized' }, { status: 401 });
  }

  try {
    const body = await request.json();
    const { contentId } = body;

    if (!contentId) {
      return NextResponse.json(
        { error: 'Content ID is required' },
        { status: 400 }
      );
    }

    // Fetch content from MongoDB
    const db = await getDb();
    const content = await db.collection('generated_content').findOne({
      _id: new ObjectId(contentId),
      userId: new ObjectId(session.user.id)
    });

    if (!content) {
      return NextResponse.json(
        { error: 'Content not found' },
        { status: 404 }
      );
    }

    // Call FastAPI for SEO analysis
    const fastapiResponse = await fetch(
      `${process.env.FASTAPI_URL}/api/seo/analyze`,
      {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          content: content.content,
          meta_title: content.metaTitle,
          meta_description: content.metaDescription,
          primary_keyword: content.primaryKeyword,
          secondary_keywords: content.secondaryKeywords || [],
          lsi_keywords: content.lsiKeywords || [],
          url_slug: content.urlSlug
        })
      }
    );

    if (!fastapiResponse.ok) {
      throw new Error('FastAPI SEO analysis failed');
    }

    const seoAnalysis = await fastapiResponse.json();

    // Store SEO analysis in MongoDB
    await db.collection('generated_content').updateOne(
      { _id: new ObjectId(contentId) },
      {
        $set: {
          seoAnalysis: {
            overallScore: seoAnalysis.overall_score,
            componentScores: seoAnalysis.component_scores,
            keywordAnalysis: seoAnalysis.keyword_analysis,
            contentStructure: seoAnalysis.content_structure,
            metaDataQuality: seoAnalysis.meta_data_quality,
            readabilityMetrics: seoAnalysis.readability_metrics,
            technicalSeo: seoAnalysis.technical_seo,
            recommendations: seoAnalysis.recommendations,
            strengths: seoAnalysis.strengths,
            weaknesses: seoAnalysis.weaknesses,
            analyzedAt: new Date()
          }
        }
      }
    );

    return NextResponse.json({
      success: true,
      seoAnalysis
    });

  } catch (error) {
    console.error('SEO analysis error:', error);
    return NextResponse.json(
      { error: 'SEO analysis failed' },
      { status: 500 }
    );
  }
}
```

**MongoDB Schema Update:**

```typescript
// generated_content collection - add seoAnalysis field
interface GeneratedContent {
  _id: ObjectId;
  userId: ObjectId;
  // ... existing fields ...
  seoAnalysis?: {
    overallScore: number;           // 0-100
    componentScores: {
      keyword_optimization: number;
      content_structure: number;
      meta_data_quality: number;
      readability_engagement: number;
      technical_seo: number;
    };
    keywordAnalysis: object;
    contentStructure: object;
    metaDataQuality: object;
    readabilityMetrics: object;
    technicalSeo: object;
    recommendations: string[];
    strengths: string[];
    weaknesses: string[];
    analyzedAt: Date;
  };
}
```

**UI Component Example (`components/seo-score-card.tsx` - NEW):**

```typescript
'use client';

import { Card, CardContent, CardHeader, CardTitle } from '@/components/ui/card';
import { Progress } from '@/components/ui/progress';
import { Badge } from '@/components/ui/badge';
import { CheckCircle2, AlertCircle, TrendingUp } from 'lucide-react';

interface SEOScoreCardProps {
  seoAnalysis: any;
}

export function SEOScoreCard({ seoAnalysis }: SEOScoreCardProps) {
  const getScoreColor = (score: number) => {
    if (score >= 80) return 'text-green-600';
    if (score >= 60) return 'text-yellow-600';
    return 'text-red-600';
  };

  const getScoreLabel = (score: number) => {
    if (score >= 90) return 'Excellent';
    if (score >= 80) return 'Good';
    if (score >= 70) return 'Fair';
    if (score >= 60) return 'Needs Work';
    return 'Poor';
  };

  return (
    <Card>
      <CardHeader>
        <CardTitle className="flex items-center justify-between">
          <span>SEO Analysis</span>
          <Badge variant={seoAnalysis.overallScore >= 80 ? 'default' : 'secondary'}>
            {getScoreLabel(seoAnalysis.overallScore)}
          </Badge>
        </CardTitle>
      </CardHeader>
      <CardContent className="space-y-6">
        {/* Overall Score */}
        <div className="text-center">
          <div className={`text-6xl font-bold ${getScoreColor(seoAnalysis.overallScore)}`}>
            {seoAnalysis.overallScore}
          </div>
          <p className="text-sm text-muted-foreground mt-2">Overall SEO Score</p>
        </div>

        {/* Component Scores */}
        <div className="space-y-3">
          {Object.entries(seoAnalysis.componentScores).map(([key, value]: [string, any]) => {
            const maxScore = key === 'keyword_optimization' ? 30 :
                            key === 'content_structure' ? 25 :
                            key === 'meta_data_quality' ? 20 :
                            key === 'readability_engagement' ? 15 : 10;
            const percentage = (value / maxScore) * 100;

            return (
              <div key={key}>
                <div className="flex justify-between text-sm mb-1">
                  <span className="capitalize">{key.replace(/_/g, ' ')}</span>
                  <span>{value.toFixed(1)} / {maxScore}</span>
                </div>
                <Progress value={percentage} className="h-2" />
              </div>
            );
          })}
        </div>

        {/* Strengths */}
        {seoAnalysis.strengths.length > 0 && (
          <div>
            <h4 className="font-semibold flex items-center gap-2 mb-2">
              <CheckCircle2 className="h-4 w-4 text-green-600" />
              Strengths
            </h4>
            <ul className="space-y-1">
              {seoAnalysis.strengths.map((strength: string, i: number) => (
                <li key={i} className="text-sm text-muted-foreground">
                  • {strength}
                </li>
              ))}
            </ul>
          </div>
        )}

        {/* Weaknesses */}
        {seoAnalysis.weaknesses.length > 0 && (
          <div>
            <h4 className="font-semibold flex items-center gap-2 mb-2">
              <AlertCircle className="h-4 w-4 text-yellow-600" />
              Areas to Improve
            </h4>
            <ul className="space-y-1">
              {seoAnalysis.weaknesses.map((weakness: string, i: number) => (
                <li key={i} className="text-sm text-muted-foreground">
                  • {weakness}
                </li>
              ))}
            </ul>
          </div>
        )}

        {/* Recommendations */}
        {seoAnalysis.recommendations.length > 0 && (
          <div>
            <h4 className="font-semibold flex items-center gap-2 mb-2">
              <TrendingUp className="h-4 w-4 text-blue-600" />
              Recommendations
            </h4>
            <ul className="space-y-2">
              {seoAnalysis.recommendations.map((rec: string, i: number) => (
                <li key={i} className="text-sm bg-muted p-2 rounded">
                  {rec}
                </li>
              ))}
            </ul>
          </div>
        )}
      </CardContent>
    </Card>
  );
}
```

## Definition of Done

- [ ] SEOAnalysisService implemented with all 5 components
- [ ] Overall score calculation with weighted components (0-100)
- [ ] Keyword analysis (density, placement, prominence) - 30 points
- [ ] Content structure analysis (headings, paragraphs, depth) - 25 points
- [ ] Meta data quality analysis (title, description, URL) - 20 points
- [ ] Readability metrics (Flesch-Kincaid) - 15 points
- [ ] Technical SEO analysis (length, stuffing, images) - 10 points
- [ ] Actionable recommendations generation
- [ ] Strengths and weaknesses identification
- [ ] FastAPI endpoint `/api/seo/analyze` created
- [ ] Next.js integration endpoint `/api/content/seo` created
- [ ] SEO analysis stored in MongoDB generated_content collection
- [ ] UI component for visual SEO score display
- [ ] Tests written for all scoring components
- [ ] Code reviewed and approved
- [ ] Merged to dev branch

## Notes

**Implementation Status**: ⏳ Depends on VIP-10204-10207 (content generation modes)

**SEO Scoring System:**

**Component Weights:**
- Keyword Optimization: 30% (most important)
- Content Structure: 25%
- Meta Data Quality: 20%
- Readability & Engagement: 15%
- Technical SEO: 10%

**Score Interpretation:**
- 90-100: Excellent (ready to publish)
- 80-89: Good (minor improvements needed)
- 70-79: Fair (moderate improvements needed)
- 60-69: Needs Work (significant improvements required)
- 0-59: Poor (major overhaul needed)

**Performance Expectations:**
- Analysis Time: 1-3 seconds per article
- Can analyze 1000+ word articles efficiently
- Recommendations generated in real-time

**Use Cases:**
- Analyze all generated content automatically after creation
- Re-analyze content after edits to track improvement
- Compare SEO scores across multiple articles
- Identify weak areas for bulk content optimization
- Track SEO score trends over time

**Future Enhancements:**
- Competitor content comparison
- Historical SEO score tracking
- A/B testing for meta data variations
- Automated content improvement suggestions
- Integration with Google Search Console

**Next Stories:**
- VIP-10210: Readability Analysis (standalone detailed readability service)
- VIP-10211: Progress Tracking (real-time job status updates)
- VIP-10212: Job Management (retry, cancel, view history)
