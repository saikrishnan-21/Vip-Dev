# VIP-10203: Ollama LLM Integration and Client Wrapper

**Story ID**: VIP-10203
**Story Type**: Story
**Epic**: E3-Content-Generation-(AI)
**Priority**: High
**Story Points**: 5
**Status**: To Do

## User Story

As a developer, I want Ollama integrated with a robust client wrapper so that I can use local LLM models for content generation with streaming, error handling, and health monitoring

## Acceptance Criteria

- [ ] **AC1**: OllamaService wrapper class with connection pooling and health checks
- [ ] **AC2**: Support for text generation with streaming responses
- [ ] **AC3**: Support for embeddings generation (nomic-embed-text model)
- [ ] **AC4**: Model availability checks and automatic fallback logic
- [ ] **AC5**: Health check endpoint verifies Ollama connectivity
- [ ] **AC6**: Comprehensive error handling with retry logic
- [ ] **AC7**: Integration tests with actual Ollama server successful

## Technical Details

**Architecture:**
- **FastAPI Role**: Manages Ollama client, handles model inference, streams responses
- **Next.js Role**: Calls FastAPI endpoints, does NOT communicate with Ollama directly
- **Ollama Server**: Remote server at http://15.134.68.145:11434
- **Models Used**:
  - `gpt-oss` - Fast model for research and SEO tasks
  - `llama3.1` - Quality model for content writing
  - `nomic-embed-text` - Embedding model for vector generation

**OllamaService Implementation (`services/ollama_service.py`):**

```python
"""
Ollama LLM Client Service
Handles all interactions with Ollama server for model inference and embeddings
"""

import aiohttp
import asyncio
import logging
from typing import Optional, List, Dict, AsyncIterator
from config import settings

logger = logging.getLogger(__name__)


class OllamaService:
    """Singleton service for Ollama LLM operations"""

    def __init__(self):
        self.base_url = settings.OLLAMA_BASE_URL
        self.session: Optional[aiohttp.ClientSession] = None
        self._models_cache: Optional[List[str]] = None
        self._cache_ttl = 300  # Cache models list for 5 minutes

    async def connect(self):
        """Initialize aiohttp session with connection pooling"""
        if self.session is None or self.session.closed:
            timeout = aiohttp.ClientTimeout(total=300)  # 5 minute timeout for long generations
            connector = aiohttp.TCPConnector(
                limit=10,  # Max 10 concurrent connections
                limit_per_host=10,
                ttl_dns_cache=300
            )
            self.session = aiohttp.ClientSession(
                timeout=timeout,
                connector=connector
            )
            logger.info(f"Connected to Ollama at {self.base_url}")

    async def disconnect(self):
        """Close aiohttp session"""
        if self.session and not self.session.closed:
            await self.session.close()
            logger.info("Disconnected from Ollama")

    async def health_check(self) -> bool:
        """
        Check if Ollama server is reachable and responsive

        Returns:
            bool: True if healthy, False otherwise
        """
        try:
            await self.connect()
            async with self.session.get(f"{self.base_url}/api/tags") as response:
                if response.status == 200:
                    logger.debug("Ollama health check: OK")
                    return True
                else:
                    logger.warning(f"Ollama health check failed: HTTP {response.status}")
                    return False
        except Exception as e:
            logger.error(f"Ollama health check failed: {e}")
            return False

    async def list_models(self, force_refresh: bool = False) -> List[Dict[str, str]]:
        """
        List available models from Ollama server

        Args:
            force_refresh: Force refresh cache

        Returns:
            List of model dictionaries with name, size, modified date
        """
        if self._models_cache and not force_refresh:
            return self._models_cache

        try:
            await self.connect()
            async with self.session.get(f"{self.base_url}/api/tags") as response:
                if response.status == 200:
                    data = await response.json()
                    self._models_cache = data.get("models", [])
                    logger.info(f"Retrieved {len(self._models_cache)} models from Ollama")
                    return self._models_cache
                else:
                    logger.error(f"Failed to list models: HTTP {response.status}")
                    return []
        except Exception as e:
            logger.error(f"Failed to list models: {e}")
            return []

    async def is_model_available(self, model_name: str) -> bool:
        """
        Check if a specific model is available

        Args:
            model_name: Name of the model (e.g., "llama3.1:8b")

        Returns:
            bool: True if model is available
        """
        models = await self.list_models()
        return any(m.get("name") == model_name for m in models)

    async def generate(
        self,
        model: str,
        prompt: str,
        system: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        stream: bool = False,
    ) -> str | AsyncIterator[str]:
        """
        Generate text using Ollama model

        Args:
            model: Model name (e.g., "llama3.1:8b")
            prompt: User prompt
            system: System prompt (optional)
            temperature: Sampling temperature (0.0-2.0)
            max_tokens: Maximum tokens to generate
            stream: Whether to stream response

        Returns:
            Generated text (str) or async iterator if streaming
        """
        await self.connect()

        # Check model availability
        if not await self.is_model_available(model):
            logger.warning(f"Model {model} not available, using fallback")
            model = settings.DEFAULT_MODEL

        payload = {
            "model": model,
            "prompt": prompt,
            "stream": stream,
            "options": {
                "temperature": temperature,
            }
        }

        if system:
            payload["system"] = system

        if max_tokens:
            payload["options"]["num_predict"] = max_tokens

        try:
            if stream:
                return self._generate_stream(payload)
            else:
                return await self._generate_non_stream(payload)

        except Exception as e:
            logger.error(f"Generation failed: {e}")
            raise

    async def _generate_non_stream(self, payload: dict) -> str:
        """Execute non-streaming generation"""
        async with self.session.post(
            f"{self.base_url}/api/generate",
            json=payload
        ) as response:
            if response.status != 200:
                error_text = await response.text()
                raise Exception(f"Ollama API error: {response.status} - {error_text}")

            data = await response.json()
            return data.get("response", "")

    async def _generate_stream(self, payload: dict) -> AsyncIterator[str]:
        """Execute streaming generation"""
        async with self.session.post(
            f"{self.base_url}/api/generate",
            json=payload
        ) as response:
            if response.status != 200:
                error_text = await response.text()
                raise Exception(f"Ollama API error: {response.status} - {error_text}")

            # Yield chunks as they arrive
            async for line in response.content:
                if line:
                    try:
                        import json
                        data = json.loads(line)
                        if "response" in data:
                            yield data["response"]
                        if data.get("done", False):
                            break
                    except json.JSONDecodeError:
                        continue

    async def generate_embeddings(
        self,
        text: str,
        model: Optional[str] = None
    ) -> List[float]:
        """
        Generate embeddings for text using Ollama

        Args:
            text: Text to embed
            model: Embedding model name (defaults to nomic-embed-text)

        Returns:
            List of floats representing the embedding vector
        """
        await self.connect()

        if model is None:
            model = settings.EMBEDDING_MODEL

        payload = {
            "model": model,
            "prompt": text
        }

        try:
            async with self.session.post(
                f"{self.base_url}/api/embeddings",
                json=payload
            ) as response:
                if response.status != 200:
                    error_text = await response.text()
                    raise Exception(f"Ollama embeddings error: {response.status} - {error_text}")

                data = await response.json()
                return data.get("embedding", [])

        except Exception as e:
            logger.error(f"Embeddings generation failed: {e}")
            raise

    async def chat(
        self,
        model: str,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        stream: bool = False,
    ) -> str | AsyncIterator[str]:
        """
        Chat completion using Ollama (alternative to generate)

        Args:
            model: Model name
            messages: List of chat messages [{"role": "user", "content": "..."}]
            temperature: Sampling temperature
            stream: Whether to stream response

        Returns:
            Generated response or async iterator
        """
        await self.connect()

        payload = {
            "model": model,
            "messages": messages,
            "stream": stream,
            "options": {
                "temperature": temperature,
            }
        }

        try:
            if stream:
                return self._chat_stream(payload)
            else:
                async with self.session.post(
                    f"{self.base_url}/api/chat",
                    json=payload
                ) as response:
                    if response.status != 200:
                        error_text = await response.text()
                        raise Exception(f"Ollama chat error: {response.status} - {error_text}")

                    data = await response.json()
                    return data.get("message", {}).get("content", "")

        except Exception as e:
            logger.error(f"Chat completion failed: {e}")
            raise

    async def _chat_stream(self, payload: dict) -> AsyncIterator[str]:
        """Execute streaming chat completion"""
        async with self.session.post(
            f"{self.base_url}/api/chat",
            json=payload
        ) as response:
            if response.status != 200:
                error_text = await response.text()
                raise Exception(f"Ollama chat error: {response.status} - {error_text}")

            async for line in response.content:
                if line:
                    try:
                        import json
                        data = json.loads(line)
                        if "message" in data:
                            yield data["message"].get("content", "")
                        if data.get("done", False):
                            break
                    except json.JSONDecodeError:
                        continue


# Global singleton instance
ollama_service = OllamaService()
```

**Configuration Updates (`config.py`):**

```python
class Settings(BaseSettings):
    # ... existing settings ...

    # Ollama Configuration
    OLLAMA_BASE_URL: str = "http://15.134.68.145:11434"
    DEFAULT_MODEL: str = "gpt-oss"
    QUALITY_MODEL: str = "llama3.1"
    EMBEDDING_MODEL: str = "nomic-embed-text"

    # Model Parameters
    MAX_TOKENS: int = 4096
    TEMPERATURE: float = 0.7
    TOP_P: float = 0.9
    FREQUENCY_PENALTY: float = 0.0
    PRESENCE_PENALTY: float = 0.0
```

**Health Check Integration (`main.py`):**

```python
from services.ollama_service import ollama_service

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Startup and shutdown events"""
    # Startup
    logger.info("Starting VIPContentAI FastAPI service...")
    await weaviate_service.connect()
    await ollama_service.connect()
    logger.info("Connected to Weaviate and Ollama")

    yield

    # Shutdown
    logger.info("Shutting down VIPContentAI FastAPI service...")
    await weaviate_service.disconnect()
    await ollama_service.disconnect()


@app.get("/health")
async def health_check():
    """Health check with all services"""
    try:
        weaviate_status = weaviate_service.is_ready()
        ollama_status = await ollama_service.health_check()
        crewai_status = crew_service._initialized

        all_healthy = weaviate_status and ollama_status and crewai_status

        return JSONResponse(
            status_code=200 if all_healthy else 503,
            content={
                "status": "healthy" if all_healthy else "degraded",
                "services": {
                    "weaviate": "up" if weaviate_status else "down",
                    "ollama": "up" if ollama_status else "down",
                    "crewai_agents": "initialized" if crewai_status else "not_initialized",
                }
            }
        )
    except Exception as e:
        logger.error(f"Health check failed: {e}")
        return JSONResponse(
            status_code=503,
            content={"status": "unhealthy", "error": str(e)}
        )
```

**Ollama Router (`routers/ollama.py` - NEW):**

```python
"""
Ollama LLM Router
Endpoints for testing and managing Ollama models
"""

from fastapi import APIRouter, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from services.ollama_service import ollama_service
from typing import Optional, List

router = APIRouter(prefix="/api/ollama", tags=["ollama"])


class GenerateRequest(BaseModel):
    """Text generation request"""
    model: str
    prompt: str
    system: Optional[str] = None
    temperature: float = 0.7
    max_tokens: Optional[int] = None
    stream: bool = False


class EmbeddingRequest(BaseModel):
    """Embedding generation request"""
    text: str
    model: Optional[str] = None


class ChatMessage(BaseModel):
    """Chat message"""
    role: str  # "user", "assistant", "system"
    content: str


class ChatRequest(BaseModel):
    """Chat completion request"""
    model: str
    messages: List[ChatMessage]
    temperature: float = 0.7
    stream: bool = False


@router.get("/models")
async def list_models(force_refresh: bool = False):
    """
    List available Ollama models

    Query params:
        force_refresh: Force refresh cached models list
    """
    try:
        models = await ollama_service.list_models(force_refresh=force_refresh)
        return {"success": True, "models": models}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/generate")
async def generate_text(request: GenerateRequest):
    """
    Generate text using Ollama model

    Supports both streaming and non-streaming responses
    """
    try:
        if request.stream:
            # Return streaming response
            async def generate_stream():
                async for chunk in await ollama_service.generate(
                    model=request.model,
                    prompt=request.prompt,
                    system=request.system,
                    temperature=request.temperature,
                    max_tokens=request.max_tokens,
                    stream=True
                ):
                    yield chunk

            return StreamingResponse(
                generate_stream(),
                media_type="text/plain"
            )
        else:
            # Return complete response
            result = await ollama_service.generate(
                model=request.model,
                prompt=request.prompt,
                system=request.system,
                temperature=request.temperature,
                max_tokens=request.max_tokens,
                stream=False
            )
            return {"success": True, "response": result}

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/embeddings")
async def generate_embeddings(request: EmbeddingRequest):
    """
    Generate embeddings for text
    """
    try:
        embeddings = await ollama_service.generate_embeddings(
            text=request.text,
            model=request.model
        )
        return {
            "success": True,
            "embeddings": embeddings,
            "dimension": len(embeddings)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/chat")
async def chat_completion(request: ChatRequest):
    """
    Chat completion using Ollama
    """
    try:
        messages = [{"role": m.role, "content": m.content} for m in request.messages]

        if request.stream:
            async def chat_stream():
                async for chunk in await ollama_service.chat(
                    model=request.model,
                    messages=messages,
                    temperature=request.temperature,
                    stream=True
                ):
                    yield chunk

            return StreamingResponse(
                chat_stream(),
                media_type="text/plain"
            )
        else:
            result = await ollama_service.chat(
                model=request.model,
                messages=messages,
                temperature=request.temperature,
                stream=False
            )
            return {"success": True, "response": result}

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

**Register Ollama Router (`main.py`):**

```python
from routers import embeddings, rss, scrape, content, ollama

# Include routers
app.include_router(embeddings.router)
app.include_router(rss.router)
app.include_router(scrape.router)
app.include_router(content.router)
app.include_router(ollama.router)  # NEW
```

**Integration Tests (`tests/test_ollama.py`):**

```python
"""Test Ollama integration"""

import pytest
from services.ollama_service import ollama_service


@pytest.mark.asyncio
async def test_ollama_health_check():
    """Test Ollama server is reachable"""
    is_healthy = await ollama_service.health_check()
    assert is_healthy == True


@pytest.mark.asyncio
async def test_list_models():
    """Test listing available models"""
    models = await ollama_service.list_models()
    assert len(models) > 0
    assert any(m["name"] == "gpt-oss" for m in models)


@pytest.mark.asyncio
async def test_generate_text():
    """Test text generation"""
    result = await ollama_service.generate(
        model="gpt-oss",
        prompt="What is fantasy football?",
        temperature=0.5,
        max_tokens=100,
        stream=False
    )
    assert len(result) > 0
    assert isinstance(result, str)


@pytest.mark.asyncio
async def test_generate_embeddings():
    """Test embeddings generation"""
    embeddings = await ollama_service.generate_embeddings(
        text="Fantasy football waiver wire targets",
        model="nomic-embed-text"
    )
    assert len(embeddings) == 768  # nomic-embed-text dimension
    assert all(isinstance(x, float) for x in embeddings)


@pytest.mark.asyncio
async def test_chat_completion():
    """Test chat completion"""
    messages = [
        {"role": "system", "content": "You are a fantasy football expert."},
        {"role": "user", "content": "Who should I start this week?"}
    ]
    result = await ollama_service.chat(
        model="gpt-oss",
        messages=messages,
        temperature=0.7,
        stream=False
    )
    assert len(result) > 0
    assert isinstance(result, str)
```

**Integration with CrewAI Agents:**

Update agent definitions to use OllamaService:

```python
# agents/researcher.py (updated)
from services.ollama_service import ollama_service

# In create_researcher_agent():
llm_config={
    "model": settings.RESEARCHER_MODEL,
    "base_url": settings.OLLAMA_BASE_URL,
    "temperature": settings.RESEARCHER_TEMPERATURE,
}
```

## Definition of Done

- [ ] OllamaService implemented in `services/ollama_service.py`
- [ ] Ollama router created with endpoints for testing (`routers/ollama.py`)
- [ ] Health check endpoint includes Ollama status
- [ ] Startup/shutdown lifecycle manages Ollama connection
- [ ] Support for text generation (streaming and non-streaming)
- [ ] Support for embeddings generation
- [ ] Support for chat completions
- [ ] Model availability checks and fallback logic
- [ ] Connection pooling with aiohttp configured
- [ ] Integration tests passing with actual Ollama server
- [ ] Error handling and logging comprehensive
- [ ] Documentation updated with Ollama endpoints
- [ ] Code reviewed and approved
- [ ] Merged to dev branch

## Notes

**Implementation Status**: ‚è≥ Depends on VIP-10201 (FastAPI setup)

**Ollama Server Details:**
- **URL**: http://15.134.68.145:11434
- **Models Available**: gpt-oss, llama3.1, nomic-embed-text
- **Connection**: Remote server (not localhost)
- **Authentication**: None required

**API Endpoints Added:**
- `GET /api/ollama/models` - List available models
- `POST /api/ollama/generate` - Text generation (streaming/non-streaming)
- `POST /api/ollama/embeddings` - Generate embeddings
- `POST /api/ollama/chat` - Chat completion

**Error Handling Strategy:**
1. **Connection Errors**: Retry with exponential backoff (max 3 attempts)
2. **Model Not Found**: Fallback to DEFAULT_MODEL (gpt-oss)
3. **Timeout**: 5-minute timeout for long generations
4. **Server Down**: Return 503 in health check, block generation requests

**Streaming Implementation:**
- Uses Server-Sent Events (SSE) pattern
- Chunks yielded as they arrive from Ollama
- Next.js can consume stream for real-time content display

**Performance Optimizations:**
- Connection pooling (max 10 concurrent connections)
- Models list cached for 5 minutes
- Async/await throughout for non-blocking I/O

**Next Stories:**
- VIP-10204: Topic-Based Generation (uses OllamaService for content generation)
- VIP-10205-10207: Other generation modes (keywords, trends, spin)
- VIP-10202: CrewAI agents will use OllamaService for LLM inference

**Testing with Ollama:**
```bash
# Manual test from terminal
curl http://15.134.68.145:11434/api/tags
curl http://15.134.68.145:11434/api/generate -d '{"model": "gpt-oss", "prompt": "Hello"}'

# Test via FastAPI docs
http://localhost:8000/docs
# Try /api/ollama/models endpoint
# Try /api/ollama/generate with sample prompt
```
